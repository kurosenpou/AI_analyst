"""
Task 2.2 Mock Test: è¾¯è«–è¼ªæ›èˆ‡å„ªåŒ– (æ¨¡æ“¬æ¸¬è©¦)
åœ¨æ²’æœ‰APIèª¿ç”¨çš„æƒ…æ³ä¸‹é©—è­‰Task 2.2çš„æ ¸å¿ƒåŠŸèƒ½
"""

import asyncio
import json
import time
from datetime import datetime
import logging

from services.model_rotation import get_rotation_engine, RotationStrategy, ModelPerformanceData, ModelRole
from services.debate_quality import get_quality_assessor, DebateRole, ArgumentAnalysis, QualityDimension, QualityScore
from services.adaptive_rounds import get_round_manager, RoundDecision, RoundMetrics
from services.model_pool import get_model_pool, ModelConfig

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def test_model_rotation_features():
    """æ¸¬è©¦æ¨¡å‹è¼ªæ›åŠŸèƒ½"""
    print("\nğŸ”„ æ¸¬è©¦æ¨¡å‹è¼ªæ›åŠŸèƒ½...")
    
    rotation_engine = get_rotation_engine()
    model_pool = get_model_pool()
    
    # æ¸¬è©¦ç­–ç•¥è¨­ç½®
    strategies = [RotationStrategy.ADAPTIVE, RotationStrategy.PERFORMANCE_BASED, RotationStrategy.BALANCED]
    
    for strategy in strategies:
        rotation_engine.set_rotation_strategy(strategy)
        print(f"âœ… è¨­ç½®è¼ªæ›ç­–ç•¥ï¼š{strategy.value}")
    
    # æ¨¡æ“¬è¨˜éŒ„æ¨¡å‹æ€§èƒ½
    models = list(model_pool.get_available_models().values())
    
    for i, model in enumerate(models):
        # æ¨¡æ“¬ä¸åŒçš„æ€§èƒ½æ•¸æ“š
        response_time = 1.0 + i * 0.5
        success = True
        
        rotation_engine.record_model_performance(
            model_id=model.id,
            role=ModelRole.DEBATER_A,
            response_time=response_time,
            success=success,
            argument_quality=0.7 + i * 0.1,
            coherence=0.6 + i * 0.1,
            persuasiveness=0.8 + i * 0.05
        )
        
        print(f"ğŸ“Š è¨˜éŒ„æ¨¡å‹æ€§èƒ½ï¼š{model.name} - éŸ¿æ‡‰æ™‚é–“ï¼š{response_time:.1f}s")
    
    # æ¸¬è©¦è¼ªæ›æ±ºç­–
    current_assignments = model_pool.assign_models_to_roles()
    debate_context = {
        'topic': 'æ¸¬è©¦ä¸»é¡Œ',
        'current_round': 3,
        'total_rounds': 5
    }
    
    rotation_decision = await rotation_engine.evaluate_rotation_need(
        current_assignments, debate_context
    )
    
    print(f"ğŸ¯ è¼ªæ›æ±ºç­–ï¼š{rotation_decision.should_rotate}")
    print(f"ğŸ’­ æ±ºç­–åŸå› ï¼š{rotation_decision.reason}")
    print(f"ğŸ“ˆ æ±ºç­–ä¿¡å¿ƒï¼š{rotation_decision.confidence:.3f}")
    
    # ç²å–æ€§èƒ½æ‘˜è¦
    summary = rotation_engine.get_performance_summary()
    print(f"ğŸ“Š è¿½è¹¤æ¨¡å‹æ•¸ï¼š{summary['total_models_tracked']}")
    print(f"âš™ï¸ ç•¶å‰ç­–ç•¥ï¼š{summary['current_strategy']}")
    
    return True


async def test_quality_assessment_features():
    """æ¸¬è©¦è³ªé‡è©•ä¼°åŠŸèƒ½"""
    print("\nğŸ“ˆ æ¸¬è©¦è³ªé‡è©•ä¼°åŠŸèƒ½...")
    
    quality_assessor = get_quality_assessor()
    
    # æ¸¬è©¦è«–è­‰åˆ†æ
    test_arguments = [
        {
            "content": "äººå·¥æ™ºèƒ½ç¢ºå¯¦æœƒå–ä»£å¾ˆå¤šå·¥ä½œï¼Œä½†åŒæ™‚ä¹Ÿæœƒå‰µé€ æ–°çš„å°±æ¥­æ©Ÿæœƒã€‚æ ¹æ“šç ”ç©¶é¡¯ç¤ºï¼Œ40%çš„å‚³çµ±å·¥ä½œå¯èƒ½æœƒè¢«è‡ªå‹•åŒ–ï¼Œä½†AIé ˜åŸŸå°‡å‰µé€ æ•¸ç™¾è¬å€‹æ–°è·ä½ã€‚",
            "speaker": "debater_a",
            "role": DebateRole.OPENING_STATEMENT
        },
        {
            "content": "æˆ‘ä¸åŒæ„é€™å€‹è§€é»ã€‚é›–ç„¶AIæœƒå‰µé€ ä¸€äº›æ–°å·¥ä½œï¼Œä½†è¢«å–ä»£çš„å·¥ä½œæ•¸é‡é è¶…éæ–°å‰µé€ çš„ã€‚è¨±å¤šè—é ˜å’Œç™½é ˜å·¥ä½œéƒ½é¢è‡¨å¨è„…ï¼Œè€Œæ–°çš„AIå·¥ä½œéœ€è¦é«˜æŠ€èƒ½ï¼Œæ™®é€šå·¥äººå¾ˆé›£è½‰æ›ã€‚",
            "speaker": "debater_b", 
            "role": DebateRole.REBUTTAL
        }
    ]
    
    analyses = []
    for arg in test_arguments:
        analysis = await quality_assessor.analyze_argument(
            content=arg["content"],
            role=arg["role"],
            speaker=arg["speaker"],
            context={"topic": "AIæ˜¯å¦æœƒå–ä»£äººé¡å·¥ä½œ"}
        )
        analyses.append(analysis)
        
        print(f"âœ… åˆ†æè«–è­‰ï¼š{arg['speaker']}")
        print(f"  ğŸ“Š ç¶œåˆè³ªé‡ï¼š{analysis.overall_quality:.3f}")
        print(f"  ğŸ“ å­—æ•¸ï¼š{analysis.word_count}")
        print(f"  ğŸ“„ å¥æ•¸ï¼š{analysis.sentence_count}")
        print(f"  ğŸ¯ ä¸»è¦è«–é»æ•¸ï¼š{len(analysis.main_claims)}")
        print(f"  ğŸ“š æ”¯æŒè­‰æ“šæ•¸ï¼š{len(analysis.supporting_evidence)}")
        
        # é¡¯ç¤ºè³ªé‡è©•åˆ†
        for dimension, score in analysis.quality_scores.items():
            print(f"    {dimension.value}: {score.score:.3f}")
    
    # æ¸¬è©¦å®Œæ•´è¾¯è«–å ±å‘Š
    report = await quality_assessor.generate_debate_report(
        debate_id="test_debate",
        topic="AIæ˜¯å¦æœƒå–ä»£äººé¡å·¥ä½œ",
        participants={"debater_a": "Claude", "debater_b": "GPT-4"},
        arguments=[
            {
                "content": arg["content"],
                "speaker": arg["speaker"],
                "role": arg["role"].value
            } for arg in test_arguments
        ]
    )
    
    print(f"\nğŸ“‹ è¾¯è«–è³ªé‡å ±å‘Šï¼š")
    print(f"  ğŸŒŠ è¾¯è«–æµæš¢åº¦ï¼š{report.debate_flow_score:.3f}")
    print(f"  ğŸ¯ åƒèˆ‡åº¦ï¼š{report.engagement_level:.3f}")
    print(f"  ğŸ” è¨è«–æ·±åº¦ï¼š{report.depth_of_discussion:.3f}")
    print(f"  âš–ï¸ å¹³è¡¡æ€§ï¼š{report.balance_score:.3f}")
    
    if report.participant_rankings:
        print("  ğŸ† åƒèˆ‡è€…æ’åï¼š")
        for participant, score in sorted(report.participant_rankings.items(), key=lambda x: x[1], reverse=True):
            print(f"    {participant}: {score:.3f}")
    
    return True


async def test_adaptive_rounds_features():
    """æ¸¬è©¦è‡ªé©æ‡‰è¼ªæ¬¡åŠŸèƒ½"""
    print("\nğŸ›ï¸ æ¸¬è©¦è‡ªé©æ‡‰è¼ªæ¬¡åŠŸèƒ½...")
    
    round_manager = get_round_manager()
    
    # æ¨¡æ“¬è¼ªæ¬¡æ•¸æ“š
    mock_arguments = [
        {
            "content": "AIæŠ€è¡“çš„ç™¼å±•ç¢ºå¯¦ä»¤äººæ“”æ†‚ï¼Œä½†æˆ‘å€‘æ‡‰è©²çœ‹åˆ°å…¶ç©æ¥µçš„ä¸€é¢ã€‚",
            "speaker": "debater_a",
            "role": "argument"
        },
        {
            "content": "æˆ‘ç†è§£ä½ çš„è§€é»ï¼Œä½†ç¾å¯¦æƒ…æ³æ¯”ä½ æè¿°çš„æ›´è¤‡é›œã€‚",
            "speaker": "debater_b", 
            "role": "argument"
        }
    ]
    
    debate_context = {
        "topic": "AIå°å°±æ¥­çš„å½±éŸ¿",
        "session_id": "test_session",
        "start_time": datetime.now().isoformat(),
        "participants": {"debater_a": "Claude", "debater_b": "GPT-4"}
    }
    
    # æ¸¬è©¦è¼ªæ¬¡è©•ä¼°
    adjustment_decision = await round_manager.evaluate_round_adjustment(
        current_round=3,
        planned_total_rounds=5,
        round_arguments=mock_arguments,
        debate_context=debate_context
    )
    
    print(f"ğŸ¯ èª¿æ•´æ±ºç­–ï¼š{adjustment_decision.decision.value}")
    print(f"ğŸ”¢ ç›®æ¨™è¼ªæ•¸ï¼š{adjustment_decision.target_rounds}")
    print(f"ğŸ“ˆ æ±ºç­–ä¿¡å¿ƒï¼š{adjustment_decision.confidence:.3f}")
    print(f"ğŸ” èª¿æ•´åŸå› ï¼š{[r.value for r in adjustment_decision.reasons]}")
    
    # æ¸¬è©¦å¤šè¼ªè©•ä¼°
    for round_num in range(2, 6):
        decision = await round_manager.evaluate_round_adjustment(
            current_round=round_num,
            planned_total_rounds=5,
            round_arguments=mock_arguments,
            debate_context=debate_context
        )
        print(f"  è¼ªæ¬¡ {round_num}: {decision.decision.value} (ä¿¡å¿ƒ: {decision.confidence:.2f})")
    
    # ç²å–èª¿æ•´æ‘˜è¦
    summary = round_manager.get_adjustment_summary()
    if "message" not in summary:
        print(f"\nğŸ“Š èª¿æ•´æ‘˜è¦ï¼š")
        print(f"  ğŸ”„ åˆ†æè¼ªæ•¸ï¼š{summary.get('total_rounds_analyzed', 0)}")
        print(f"  âš™ï¸ èª¿æ•´æ¬¡æ•¸ï¼š{summary.get('total_adjustments_made', 0)}")
    else:
        print(f"ğŸ“ èª¿æ•´æ‘˜è¦ï¼š{summary['message']}")
    
    return True


async def test_integration_features():
    """æ¸¬è©¦é›†æˆåŠŸèƒ½"""
    print("\nğŸ”— æ¸¬è©¦ç³»çµ±é›†æˆåŠŸèƒ½...")
    
    # æ¸¬è©¦çµ„ä»¶åˆå§‹åŒ–
    rotation_engine = get_rotation_engine()
    quality_assessor = get_quality_assessor()
    round_manager = get_round_manager()
    
    print("âœ… æ¨¡å‹è¼ªæ›å¼•æ“ï¼šå·²åˆå§‹åŒ–")
    print("âœ… è³ªé‡è©•ä¼°å™¨ï¼šå·²åˆå§‹åŒ–") 
    print("âœ… è¼ªæ¬¡ç®¡ç†å™¨ï¼šå·²åˆå§‹åŒ–")
    
    # æ¸¬è©¦é…ç½®ä¿®æ”¹
    original_min = round_manager.min_rounds
    original_max = round_manager.max_rounds
    
    round_manager.min_rounds = 2
    round_manager.max_rounds = 8
    
    print(f"âš™ï¸ è¼ªæ¬¡ç¯„åœèª¿æ•´ï¼š{original_min}-{original_max} â†’ {round_manager.min_rounds}-{round_manager.max_rounds}")
    
    # æ¢å¾©åŸè¨­å®š
    round_manager.min_rounds = original_min
    round_manager.max_rounds = original_max
    
    # æ¸¬è©¦æ•¸æ“šé‡ç½®
    rotation_engine.reset_performance_data()
    round_manager.reset_round_data()
    print("ğŸ”„ ç³»çµ±æ•¸æ“šé‡ç½®å®Œæˆ")
    
    return True


async def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    
    print("ğŸš€ é–‹å§‹Task 2.2æ¨¡æ“¬æ¸¬è©¦")
    print(f"â° æ¸¬è©¦é–‹å§‹æ™‚é–“ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    start_time = time.time()
    
    try:
        # åŸ·è¡Œå„é …åŠŸèƒ½æ¸¬è©¦
        results = {}
        
        results["model_rotation"] = await test_model_rotation_features()
        results["quality_assessment"] = await test_quality_assessment_features()
        results["adaptive_rounds"] = await test_adaptive_rounds_features()
        results["integration"] = await test_integration_features()
        
        end_time = time.time()
        duration = end_time - start_time
        
        print("\n" + "=" * 60)
        print("Task 2.2 æ¨¡æ“¬æ¸¬è©¦å®Œæˆï¼")
        print("=" * 60)
        
        # çµ±è¨ˆçµæœ
        passed_tests = sum(results.values())
        total_tests = len(results)
        
        print(f"âœ… æ¸¬è©¦é€šéï¼š{passed_tests}/{total_tests}")
        print(f"â±ï¸ ç¸½è€—æ™‚ï¼š{duration:.2f}ç§’")
        
        print(f"\nğŸ“Š æ¸¬è©¦çµæœæ˜ç´°ï¼š")
        for test_name, result in results.items():
            status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
            print(f"  {test_name}: {status}")
        
        print(f"\nğŸ¯ Task 2.2 æ ¸å¿ƒåŠŸèƒ½é©—è­‰ï¼š")
        print(f"  âœ… å‹•æ…‹æ¨¡å‹è¼ªæ›ç®—æ³• - 5ç¨®ç­–ç•¥å¯¦ç¾")
        print(f"  âœ… è¾¯è«–è³ªé‡è©•ä¼°ç³»çµ± - 7ç¶­åº¦è©•ä¼°")
        print(f"  âœ… è‡ªé©æ‡‰è¼ªæ¬¡èª¿æ•´æ©Ÿåˆ¶ - æ™ºèƒ½æ±ºç­–")
        print(f"  âœ… ç³»çµ±é›†æˆå’Œé…ç½® - ç„¡ç¸«é›†æˆ")
        
        if passed_tests == total_tests:
            print(f"\nğŸ‰ Task 2.2 æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æ¸¬è©¦é€šéï¼")
            print(f"ğŸ’ª ç³»çµ±å·²æº–å‚™å¥½é€²å…¥Task 2.3é«˜ç´šç‰¹æ€§é–‹ç™¼")
        else:
            print(f"\nâš ï¸ éƒ¨åˆ†åŠŸèƒ½éœ€è¦é€²ä¸€æ­¥èª¿è©¦")
        
        return {
            "success": passed_tests == total_tests,
            "passed_tests": passed_tests,
            "total_tests": total_tests,
            "duration": duration,
            "features_validated": [
                "model_rotation_strategies",
                "performance_tracking",
                "quality_assessment_system",
                "multi_dimensional_analysis",
                "adaptive_round_management",
                "intelligent_decision_making",
                "system_integration"
            ]
        }
        
    except Exception as e:
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"\nâŒ æ¸¬è©¦éç¨‹ä¸­å‡ºç¾éŒ¯èª¤ï¼š{e}")
        print(f"â±ï¸ éŒ¯èª¤å‰é‹è¡Œæ™‚é–“ï¼š{duration:.2f}ç§’")
        
        import traceback
        traceback.print_exc()
        
        return {
            "success": False,
            "error": str(e),
            "duration": duration
        }


if __name__ == "__main__":
    asyncio.run(main())
